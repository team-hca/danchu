# A302 햅피 콤퓨타

<hr>

## 23.09.04(월)
위키피디아 데이터까지 받아왔다

다음 차례는 이제 명사 추출인데, wikitext_nouns_extractor가 작동을 하지 않는다
빈 파일만 뱉어낸다

고치고있다

<hr>

## 23.09.05(화)

### 1. KoNLPy - Hannanum 

### 2. KoNLPy - okt

한나눔은 분석 길이 제한이 있어서 시간이 매우매우 오래걸린다
하지만 순서대로 볼 수 있어서 데이터 분석 중간에 끊길 경우 이어서 할 수 있을 것 같다
그리고 중간중간에 계속 오류가 난다(ArrayIndexOutOfBound)
분석 가능 길이를 넘어서 그런 것 같다
그리고 한자랑 이상한 언어들도 섞여서 출력된다

okt는 한글만 잘 잘라준다. 숫자도 자른다 한자도 자른다
중간중간에 조금 끊기지만 이 정도는 괜찮은 편이다
어차피 다른 것도 잘린다
하지만 500MB씩 끊을 수 있어서 속도 면에서 좀 더 나아 보인다

<hr>

## 23.09.06(수)

### 1. 바른 API

### 2. 바른 + ray

바른이 깔끔하게 잘 나오는데, 너무 느리다
ray를 통해 멀티쓰레드로 작업하고 싶은데 지금 직렬화 오류에 직면했다

<hr>

## 23.09.07(목)

### 1. RAY를 이용한 분산 처리 결과

레이 사용 전

![image](/uploads/bea64f6e6750d6201935c1368e6c40fe/image.png){: width="70%"}

</br>

레이 사용 후

![image](/uploads/9595257fde18354b4180b797e44f5279/image.png){: width="70%"}

같은 라인(19505 라인)을 처리 할 때 속도 체감 약 2.2배

용량 : 10,674 KB(약 10.44MB)

시간 변화 : 382.43 -> 172.83(2.2배)

CPUS = 16으로 사용

<hr>

## 23.09.07(금)

### 1. 하둡 윈도우 환경

노션 개발일지에 정리

하둡 버전 3.3.1

[사용 방법 - 노션](https://invented-halloumi-54f.notion.site/_-4041b7e175d14cbfb1b60e22f00e07aa?pvs=4)

etc\hadoop 폴더 내에
hadoop-env.cmd 폴더에서
JavaPath를 본인의 자바 환경으로 설정해줘야하고

환경 변수 내에서 환경 변수 설정도 꼭 해줘야 한다!

실행은 sbin폴더로 이동해서

start-dfs.cmd
와
start-yarn.cmd
를 입력해서 데이터노드, 네임노드, 리소스매니저, 노드매니저를 실행시켜야 한다

## 23.09.11(월)

### 1. 하둡 실행 방법

bin 폴더에 필요한 jar 파일을 넣는다

그리고 출력 파일을

bin 폴더 내의 관리자 권한이 실행된 cmd 창에서 명령어로 초기화해준다

``` bin>hdfs dfs -rm -R /output ``` 

다음으로는 파일을 넣어준다. 이것도 bin에 있어야한다(아니면 경로잡아줘야함)

```bin>hdfs dfs -put <파일이름> <경로>```


실행 명령어는 다음과 같다

``` bin>yarn jar <jar파일이름.jar> <실행 클래스명(Driver 내에 있음)> <입력 파일 위치 및 이름> <출력 파일> ```

이후 다른 위치로 옮기는 명령어로 옮겨준다

``` bin>hdfs dfs -copyToLocal <hdfs에 저장된 경로 및 이름> <저장할 로컬 경로> ```

## 23.09.14(목)

### 1. 하둡 EC2 서버로 옮기기

Ubuntu 서버에서 돌리기 실행 - wordcount까지 수행

## 23.09.15(금)

### 1. 하둡 워드카운팅 결과 저장

하둡 워드카운팅 결과 두 글자 이상으로 추려서 상위 100개 단어 저장
1. 맵 함수에서 단어를 쪼갤 때, 정규식을 사용해 "[\\n,]+" 줄 바꿈과 ,를 기준으로 모두 끊는다
2. 끊은 단어를 String 배열로 담은 뒤 배열 인자를 반복문 돌리면서 길이가 1 이하인 애들은 continue 진행한다

## 23.09.17(일)

### 1. 주말 작업 내용 정리

1. 파이썬 명령어 실행으로 자동 저장 진행(23.09.15(금) ~ 09.16(토))
2. 하둡 서버 오류로 인한 버그 픽스 진행(포트 사용 오류로 인해 모두 재시작(~23.09.17(일)))
3. 명령어를 통한 Top100 뉴스 저장 자동화 진행(~23.09.17(일))

### 2. 실행 순서

(하둡 서버 가동을 전제로)
1. 파이썬 명사 가져오기 : ```python3 ~/extractor.py``` - extractor 파일 실행
2. 하둡으로 파일 넣기 : ```hdfs dfs -put $HADOOP_HOME/bin/news20230917.txt /``` - put 해주기(여기서 put뒤에 날짜가 계속 바뀌어야 하는데 가능할지?)
3. Wordcount java 파일 수행 : ```yarn jar $HADOOP_HOME/bin/Wordcount.jar wordcount news extract``` - 하둡으로 java파일 실행

### 3. 추후 작업 소요

1. 하둡에 파일 넣는 명령어 실행시 날짜를 계속 바꾸어주어야 하는 소요. 자동화시 가능한지? -> 명사 추출 기능도 java로 옮기기(Mecab)
